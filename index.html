<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reasoning Grasping via Multimodal Large Language Model">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reasoning Grasping via Multimodal Large Language Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reasoning Grasping via Multimodal Large Language Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=GdYgso8AAAAJ&hl=en">Shiyu Jin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.jinxuanxu.com/">Jinxuan Xu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=L8aba_MAAAAJ&hl=en">Yutian Lei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.unc.edu/~zlj/">Liangjun Zhang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Baidu Research,</span>
            <span class="author-block"><sup>2</sup>Rutgers University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CoRL 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=KPcX4jetMw"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://reasoning-grasping.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h2 class="title">1-min Presentation</h2> -->
          <video width="100%" controls>
            <source src="./static/videos/corl-2024-reasoning-grasping_with_subtitles.mp4" type="video/mp4">
            <div class="content justified-text">
              <p>
                1-min Presentation
              </p>
            </div>
          </video>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content justified-text">
          <p>
            Despite significant progress in robotic systems for operation within human-centric environments, existing models still heavily rely on explicit human commands to identify and manipulate specific objects. This limits their effectiveness in environments where understanding and acting on implicit human intentions are crucial. In this study, we introduce a novel task: reasoning grasping, where robots need to generate grasp poses based on indirect verbal instructions or intentions. 
            To accomplish this, we propose an end-to-end reasoning grasping model that integrates a multimodal Large Language Model (LLM) with a vision-based robotic grasping framework.
            In addition, we present the first reasoning grasping benchmark dataset generated from the GraspNet-1 billion, incorporating implicit instructions for object-level and part-level grasping.
            Our results show that directly integrating CLIP or LLaVA with the grasp detection model performs poorly on the challenging reasoning grasping tasks, while our proposed model demonstrates significantly enhanced performance both in the reasoning grasping benchmark and real-world experiments. 
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivations and Application Scenarios</h2>
        <div class="content justified-text">
          <p>
            Motivations 
            <br>
            Advance robotic systems should not only understand direct human commands but to also possess the reasoning capability to interpret implicit instructions. These implicit instructions can provide a general description of users’ needs, requiring the robot to infer the grasping target on its own, without explicitly naming the object.
            <br><br>
            Application Scenarios
            <br>
1. Users cannot visually confirm the robot’s surroundings: For example, blind people can
instruct the robot to “find a sweater in a warm color that matches the pants”. The robot could
also be asked to “find something to hold these papers together”, where it might choose from
a stapler, paper clips, or binder clips based on availability.
<br>
2. Tasks that require internal knowledge for decision making: Robots can utilize their internal
knowledge for tasks like sorting in recycling facilities. They could be instructed to “sort
materials that are recyclable and compostable”, identifying items based on texture and
material type, which facilitates sorting without the need to know whether the objects are
recyclable or not.
<br>
3. Naming objects is impractical or complex: In some situations, explicitly naming objects is
impractical or the names are complex and difficult to remember. For instance, telling a robot, “I
need my morning medicine”, allows the robot to use its routine knowledge to fetch the correct
medication without the user needing to recall specific drug names, which are usually long and
complex.
<br>
4. Managing multiple items: Explicitly naming multiple objects can be cumbersome and
inefficient. In emergency medical situations, a medical robot might be instructed to “gather all
items necessary for suturing wounds”. It would need to discern which tools and supplies are
relevant, such as needles, thread, and antiseptics, based on the medical context provided. For
general tidying tasks in home organization, a home robot could be instructed with phrases like
“The living room is messy, tidy up by picking up toys”, allowing the robot to identify and collect
toys without naming each item individually (e.g., teddy bear, toy train, etc.).
<br>

          </p>
        </div>
      </div>
    </div>
</section>




<section class="hero teaser section-spacing">
  <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title">Introduction</h2>
            <img id="teaser" src="./static/images/first_figure.png" alt="Teaser Image" height="100%">
            <div class="content justified-text">
              <p>
                Our contributions are: 1.Introduce Reasoning Grasping - Generating grasping poses from implicit instructions. 2.Propose a multi-modal large language model that can finish the reasoning grasping task. 3.Develop a reasoning grasping dataset that incorporates implicit instructions for both objectlevel and part-level grasping.
              </p>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="hero teaser section-spacing">
  <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title">Framework</h2>
      <img id="teaser" src="./static/images/revise_framework.png" alt="Teaser Image" height="100%">
      <div class="content justified-text">
        <p>
          Framework of the proposed reasoning grasping model. This model processes visual images v and textual instructions t to output the grasp pose g for the specified target object or part. The embeddings of the grasp target are passed to the grasping module for grasping poses detection.
        </p>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser section-spacing">
  <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title">Reasoning Grasping Dataset</h2>
      <img id="teaser" src="./static/images/dataset_example1.jpg" alt="Teaser Image" height="100%">
      <div class="content ">
        <p>
          We extend the GraspNet-1B dataset with reasoning instructions as well as object parts grasping annotations.
        </p>
      </div>
      <img id="teaser" src="./static/images/different_datasets.png" alt="Teaser Image" height="100%">
      <div class="content ">
        <p>
          Comparison with different datasets
        </p>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser section-spacing">
  <div class="container is-max-desktop">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title">Experiments</h2>
      <img id="teaser" src="./static/images/experiments_on_dataset.png" alt="Teaser Image" height="100%">
      <div class="content ">
        <p>
          Grasp prediction accuracy on the reasoning grasping dataset.
        </p>
      </div>
      <img id="teaser" src="./static/images/real_exp.jpg" alt="Teaser Image" height="100%">
      <img id="teaser" src="./static/images/real_exp_results.png" alt="Teaser Image" height="100%">
      <div class="content ">
        <p>
          Comparison with different datasets
        </p>
      </div>
      </div>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jin2024reasoning,
  author    = {Jin, Shiyu and Xu, Jinxuan and Lei, Yutian and Zhang, Liangjun},
  title     = {Reasoning grasping via multimodal large language model},
  journal   = {Conference on Robot Learning},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
